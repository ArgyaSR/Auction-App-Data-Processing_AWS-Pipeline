# Auction Web App | Real-Time Data Pipeline with Kafka, Spark & AWS

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Apache Kafka](https://img.shields.io/badge/Kafka-3.6-orange.svg)](https://kafka.apache.org/)
[![Apache Spark](https://img.shields.io/badge/Spark-3.5-yellow.svg)](https://spark.apache.org/)
[![AWS](https://img.shields.io/badge/AWS-Free.svg)](https://aws.amazon.com/free/)

A production-grade, end-to-end data pipeline for processing real-time auction bids and batch analytics. This project presents a prime integration of cloud and open-source tools for scalable ETL processes, streaming data, and data visualization based on client requirements.

---

# Table of Contents

- [Business Case](#business-case)
- [Technical Requirements](#technical-requirements)
- [Tools Used](#tools-used)
- [High Level Architecture](#high-level-architecture)
- [The Source Dataset](#the-source-dataset)
- [Database Schema](#database-schema)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Data Flow](#data-flow)
- [Configuration](#configuration)
- [Deployment](#deployment)
- [Testing](#testing)
- [Monitoring](#monitoring)
- [Key Capabilities](#key-capabilities)
- [Contributing](#contributing)
- [Conclusion](#conclusion)

---

# Business Case

The client runs an auction web app for a variety of goods including antiques, art pieces, vintage clothing, jewelry, electronics, and several other miscellaneous collectibles. On the initial launch of the app, the data from the web app was simply stored in a PostgreSQL database and analytics were run ad-hoc. However, due to a significant increase in users on the application a year later, the infrastructure was not sufficient to handle the load which led to:

- **Application downtimes** during peak bidding periods
- **Slow landing pages** affecting user experience
- **Database bottlenecks** from concurrent bid writes
- **Delayed analytics** preventing real-time business decisions
- **Lost revenue** from failed transactions during high-traffic auctions

Now the client needs a solution to efficiently process all batch data (for physical auctions done on paper) and real-time streaming data (several thousands of online bids are logged per minute with Point of Sale transactions included). The solution should also include visualization of the records/metrics on their web application and also on their internal BI Dashboard while the application remains performant.

**Key Business Objectives:**
1. Handle 10,000+ concurrent bid events per second without performance degradation
2. Provide real-time price updates to auction participants (< 500ms latency)
3. Enable daily analytics for business intelligence and seller reporting
4. Implement fraud detection for suspicious bidding patterns
5. Maintain 99.9% uptime during auction periods

---

# Technical Requirements

| Functional ðŸŸ¢ | Non-Functional ðŸ”µ |
| ------------- | ----------------- |
| The system shall ingest real-time bid events from the auction web application via Kafka topics. | The system shall be scalable to handle up to 10,000 bid events per second without significant degradation in performance. |
| The system shall process batch data from physical auctions (CSV/JSON files) stored in cloud storage daily. | The system shall achieve end-to-end latency of less than 500ms for streaming bid validation and price updates. |
| The system shall validate bid amounts against current auction prices and reject invalid bids in real-time. | The system shall support horizontal scaling by adding Kafka partitions, Spark workers, and database read replicas. |
| The system shall detect and flag potentially fraudulent bidding patterns (e.g., bid sniping, unusual increment sizes). | Access to sensitive data (e.g., user details, financial transactions) shall be role-based with encryption at rest and in transit. |
| The system shall maintain a medallion data architecture (Bronze/Silver/Gold layers) for data quality and lineage. | The system shall have 99.9% availability to ensure auction operations continue uninterrupted. |
| The system shall calculate daily analytics including revenue metrics, auction performance, bidder segmentation, and seller rankings. | Backup and recovery processes shall enable data restoration within 1 hour in case of system failure. |
| The system shall provide real-time dashboards showing active auctions, current bids, and key performance metrics. | The system shall be deployable within AWS Free Tier constraints for development and demonstration purposes. |
| The system shall notify administrators of ingestion failures, processing errors, or system anomalies via alerts. | The system's codebase and infrastructure shall be fully documented with Architecture Decision Records (ADRs). |

---

# Tools Used

**Programming Language** - [Python](https://www.python.org/) ![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python&logoColor=white)

**Cloud Infrastructure** - [Amazon Web Services (AWS)](https://aws.amazon.com/) ![AWS](https://img.shields.io/badge/AWS-Free?logo=amazonaws&logoColor=white)

**Event Streaming** - [Apache Kafka](https://kafka.apache.org/) ![Kafka](https://img.shields.io/badge/Kafka-3.6-black?logo=apachekafka&logoColor=white)

**Stream Processing** - [Apache Spark Structured Streaming](https://spark.apache.org/) ![Spark](https://img.shields.io/badge/Spark-3.5-orange?logo=apachespark&logoColor=white)

**Batch Processing** - [Apache Spark](https://spark.apache.org/) ![Spark](https://img.shields.io/badge/Spark-3.5-orange?logo=apachespark&logoColor=white)

**Workflow Orchestration** - [Apache Airflow](https://airflow.apache.org/) ![Airflow](https://img.shields.io/badge/Airflow-2.7-teal?logo=apacheairflow&logoColor=white)

**Containerization** - [Docker](https://www.docker.com/) ![Docker](https://img.shields.io/badge/Docker-24.0-blue?logo=docker&logoColor=white)

**Object Storage** - [AWS S3](https://aws.amazon.com/s3/) / [LocalStack](https://localstack.cloud/) ![S3](https://img.shields.io/badge/S3-Bronze%20Layer-green?logo=amazons3&logoColor=white)

**Operational Database** - [PostgreSQL](https://www.postgresql.org/) ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue?logo=postgresql&logoColor=white)

**Analytics Database** - [DuckDB](https://duckdb.org/) ![DuckDB](https://img.shields.io/badge/DuckDB-OLAP-yellow?logo=duckdb&logoColor=black)

**Data Visualization** - [Metabase](https://www.metabase.com/) ![Metabase](https://img.shields.io/badge/Metabase-Dashboards-blue?logo=metabase&logoColor=white)

**Infrastructure as Code** - [Terraform](https://www.terraform.io/) ![Terraform](https://img.shields.io/badge/Terraform-1.6-purple?logo=terraform&logoColor=white)

**CI/CD** - [GitHub Actions](https://github.com/features/actions) ![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-CI%2FCD-blue?logo=githubactions&logoColor=white)

---

# High Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              DATA SOURCES                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Auction API   â”‚   IoT/POS       â”‚   Batch Files   â”‚   Historical Data     â”‚
â”‚   (Real-time)   â”‚   Devices       â”‚   (CSV/JSON)    â”‚   (S3)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                 â”‚                   â”‚
         â–¼                 â–¼                 â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INGESTION LAYER                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      Apache Kafka (KRaft Mode)                       â”‚    â”‚
â”‚  â”‚  Topics: auction.bids | auction.items | auction.users | auction.txn â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      STREAM PROCESSING        â”‚ â”‚           BATCH PROCESSING                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Spark Structured       â”‚  â”‚ â”‚  â”‚  Apache Spark (Batch Jobs)          â”‚  â”‚
â”‚  â”‚  Streaming              â”‚  â”‚ â”‚  â”‚  - Daily aggregations               â”‚  â”‚
â”‚  â”‚  - Real-time bid        â”‚  â”‚ â”‚  â”‚  - Historical analytics             â”‚  â”‚
â”‚  â”‚    validation           â”‚  â”‚ â”‚  â”‚  - ML feature engineering           â”‚  â”‚
â”‚  â”‚  - Price updates        â”‚  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚  - Fraud detection      â”‚  â”‚ â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  Orchestrated by Apache Airflow           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                                       â”‚
                â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            STORAGE LAYER                                     â”‚
â”‚                         (Medallion Architecture)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   BRONZE     â”‚â”€â”€â”€â–¶â”‚   SILVER     â”‚â”€â”€â”€â–¶â”‚    GOLD      â”‚                   â”‚
â”‚  â”‚   (S3)       â”‚    â”‚ (PostgreSQL) â”‚    â”‚  (DuckDB/    â”‚                   â”‚
â”‚  â”‚  Raw Events  â”‚    â”‚   Cleaned    â”‚    â”‚   Analytics) â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          VISUALIZATION LAYER                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚       Metabase          â”‚    â”‚      Application API                â”‚     â”‚
â”‚  â”‚   (BI Dashboards)       â”‚    â”‚   (Real-time auction data)          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Architecture Choices Explained

**Apache Kafka (KRaft Mode)** was chosen as the streaming platform because it acts as a massive, durable buffer between data sources and processors. The front-end application and IoT devices can fire events into Kafka at an extremely high rate without waiting for them to be processed or stored in a database. This prevents the primary application database from being overwhelmed by write requestsâ€”a major cause of downtime in simpler architectures. Kafka is built to handle millions of messages per second, making thousands per minute well within its capacity. KRaft mode eliminates the need for Zookeeper, simplifying operations.

**Apache Spark** was selected over Apache Flink for processing because:
1. Spark's PySpark API is more mature and full-featured for Python development
2. The micro-batch latency (~100ms) is sufficient for auction systems
3. Same DataFrame API works for both batch and streaming, reducing code complexity

**PostgreSQL** serves as the operational database (Silver layer) because it provides ACID compliance for transaction integrity, supports complex queries for application needs, and integrates seamlessly with both Spark and visualization tools.

**AWS S3** (or LocalStack for local development) provides the Bronze layer storage because it offers virtually unlimited, low-cost storage for raw event data with built-in durability and lifecycle management.

**DuckDB** powers the Gold layer analytics because it provides blazing-fast OLAP queries without the cost of a dedicated data warehouse, perfect for Free Tier deployments.

**Metabase** was chosen for visualization because it's open-source, provides beautiful dashboards out of the box, and connects directly to PostgreSQL and DuckDB without complex configuration.

**Docker Compose** enables local development that mirrors production, allowing the entire stack to run on a single machine for testing and demonstration.

---

# The Source Dataset

Since this is a demonstration project, the source data is **synthetically generated** using a custom data generator that produces realistic auction data patterns. The generator implements research-based auction behaviors:

**Bidding Patterns (Based on eBay Research):**
- 32% of bids occur in the final minute (bid sniping)
- 17% occur in the 1-5 minute window before close
- 8% occur in the 5-60 minute window
- 43% are distributed throughout the auction

**eBay-Standard Bid Increments:**
| Current Price | Bid Increment |
|--------------|---------------|
| $0.01 - $0.99 | $0.05 |
| $1.00 - $4.99 | $0.25 |
| $5.00 - $24.99 | $0.50 |
| $25.00 - $99.99 | $1.00 |
| $100.00 - $249.99 | $2.50 |
| $250.00 - $499.99 | $5.00 |
| $500.00 - $999.99 | $10.00 |
| $1,000.00 - $2,499.99 | $25.00 |
| $2,500.00 - $4,999.99 | $50.00 |
| $5,000.00+ | $100.00 |

**Data Entities Generated:**

| Entity | Description | Volume |
|--------|-------------|--------|
| **Users** | Bidders and sellers with ratings, locations, verification status | 1,000+ |
| **Items** | Auction listings across 6 categories (Antiques, Art, Electronics, Jewelry, Collectibles, Vintage Clothing) | 5,000+ |
| **Bids** | Individual bid events with timestamps, types (manual/proxy/snipe), and fraud scores | 100,000+ |
| **Transactions** | Completed sales with payment and shipping details | Based on auction completions |

View the data generator implementation in [`src/data_generator/`](src/data_generator/).

---

# Database Schema

Considering this will be a **heavy write** data pipeline with frequent updates to the database (thousands of bids per minute), and a moderate number of users querying results (analysts, application users), we implement a **Medallion Architecture** with three distinct layers:

## Medallion Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     BRONZE      â”‚â”€â”€â”€â”€â–¶â”‚     SILVER      â”‚â”€â”€â”€â”€â–¶â”‚      GOLD       â”‚
â”‚   (Raw Data)    â”‚     â”‚   (Validated)   â”‚     â”‚  (Aggregated)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ S3/LocalStack â”‚     â”‚ â€¢ PostgreSQL    â”‚     â”‚ â€¢ PostgreSQL    â”‚
â”‚ â€¢ JSON/Parquet  â”‚     â”‚ â€¢ Normalized    â”‚     â”‚ â€¢ Denormalized  â”‚
â”‚ â€¢ Immutable     â”‚     â”‚ â€¢ FK Relations  â”‚     â”‚ â€¢ Pre-computed  â”‚
â”‚ â€¢ 90+ day retainâ”‚     â”‚ â€¢ Indexed       â”‚     â”‚ â€¢ Dashboard-    â”‚
â”‚                 â”‚     â”‚ â€¢ Validated     â”‚     â”‚   ready         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Silver Layer Entity Relationship

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    users     â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ user_id (PK) â”‚
                    â”‚ username     â”‚
                    â”‚ email        â”‚
                    â”‚ rating       â”‚
                    â”‚ is_verified  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                â”‚                â”‚
          â–¼                â–¼                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    items     â”‚ â”‚     bids     â”‚ â”‚ transactions â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ item_id (PK) â”‚ â”‚ bid_id (PK)  â”‚ â”‚ txn_id (PK)  â”‚
   â”‚ seller_id(FK)â”‚ â”‚ auction_id   â”‚ â”‚ auction_id   â”‚
   â”‚ title        â”‚ â”‚ bidder_id(FK)â”‚ â”‚ seller_id(FK)â”‚
   â”‚ category     â”‚ â”‚ bid_amount   â”‚ â”‚ buyer_id(FK) â”‚
   â”‚ start_price  â”‚ â”‚ bid_type     â”‚ â”‚ final_price  â”‚
   â”‚ current_priceâ”‚ â”‚ fraud_score  â”‚ â”‚ platform_fee â”‚
   â”‚ status       â”‚ â”‚ is_winning   â”‚ â”‚ is_completed â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Gold Layer Analytics Tables

| Table | Purpose | Refresh Frequency |
|-------|---------|-------------------|
| `daily_revenue` | Daily gross/platform/seller revenue metrics | Daily |
| `auction_performance` | Per-auction stats (bid counts, price increase %) | Daily |
| `bidder_analytics` | Bidder segmentation (power/active/casual/new) | Daily |
| `seller_rankings` | Seller tiers and conversion rates | Daily |
| `hourly_activity` | Bidding patterns by hour for trend analysis | Hourly |
| `fraud_summary` | Daily fraud detection metrics | Daily |

View the complete schema definitions in [`sql/`](sql/).

---

# Quick Start

### Prerequisites

- Docker Desktop 4.0+ with WSL2 backend (Windows)
- Python 3.11+
- Make (optional, for convenience commands)
- AWS CLI (for cloud deployment)

### One-Command Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/auction-data-pipeline.git
cd auction-data-pipeline

# Copy environment template
cp .env.example .env

# Start all services
make up

# Or without Make:
docker compose up -d
```

### Access Points

| Service | URL | Credentials |
|---------|-----|-------------|
| Kafka UI | http://localhost:8080 | - |
| Airflow | http://localhost:8081 | airflow / airflow |
| Metabase | http://localhost:3000 | Setup on first visit |
| Spark UI | http://localhost:4040 | - (during job execution) |
| PostgreSQL | localhost:5432 | auction / auction123 |

### Generate Sample Data

```bash
# Start the data generator (produces to Kafka)
make generate-data

# Or run directly:
python -m src.data_generator.kafka_producer --events 10000 --rate 100
```

### Run the Pipeline

```bash
# Start Spark streaming job
make run-streaming

# Trigger batch processing via Airflow UI or:
make run-batch
```

---

# Project Structure

```
auction-data-pipeline/
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ docker-compose.yml              # All services orchestration
â”œâ”€â”€ Makefile                        # Convenience commands
â”œâ”€â”€ .env.example                    # Environment template
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ pyproject.toml                  # Project metadata
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_generator/             # Synthetic data generation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ generator.py            # Core data generation logic
â”‚   â”‚   â”œâ”€â”€ schemas.py              # Data models (Pydantic)
â”‚   â”‚   â”œâ”€â”€ kafka_producer.py       # Stream events to Kafka
â”‚   â”‚   â””â”€â”€ batch_generator.py      # Generate batch files
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/                  # Data ingestion layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ kafka_consumer.py       # Base Kafka consumer
â”‚   â”‚   â””â”€â”€ s3_ingestion.py         # S3 batch ingestion
â”‚   â”‚
â”‚   â”œâ”€â”€ transformation/             # Processing layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ spark_streaming.py      # Spark Structured Streaming
â”‚   â”‚   â””â”€â”€ batch_processing.py     # Spark batch jobs
â”‚   â”‚
â”‚   â””â”€â”€ serving/                    # Data serving layer
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ postgres_loader.py      # Load to PostgreSQL
â”‚       â””â”€â”€ duckdb_analytics.py     # DuckDB analytics queries
â”‚
â”œâ”€â”€ dags/                           # Airflow DAGs
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ batch_processing_dag.py     # Daily batch ETL
â”‚
â”œâ”€â”€ sql/                            # Database schemas
â”‚   â”œâ”€â”€ init.sql                    # Initial setup
â”‚   â”œâ”€â”€ silver_schema.sql           # Cleaned data schema
â”‚   â””â”€â”€ gold_schema.sql             # Analytics schema
â”‚
â”œâ”€â”€ terraform/                      # Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf                     # Main configuration
â”‚   â”œâ”€â”€ variables.tf                # Input variables
â”‚   â”œâ”€â”€ outputs.tf                  # Output values
â”‚   â””â”€â”€ modules/                    # Reusable modules
â”‚       â”œâ”€â”€ s3/
â”‚       â”œâ”€â”€ rds/
â”‚       â””â”€â”€ iam/
â”‚
â”œâ”€â”€ tests/                          # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                 # Pytest fixtures
â”‚   â”œâ”€â”€ test_generator.py           # Data generator tests
â”‚   â”œâ”€â”€ test_transformations.py     # Transform logic tests
â”‚   â””â”€â”€ integration/                # Integration tests
â”‚       â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ config/                         # Configuration files
â”‚   â””â”€â”€ spark-defaults.conf         # Spark configuration
â”‚
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md             # Detailed architecture
â”‚   â””â”€â”€ adr/                        # Architecture Decision Records
â”‚       â”œâ”€â”€ 001-kafka-over-kinesis.md
â”‚       â””â”€â”€ 002-spark-over-flink.md
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ ci.yml                  # GitHub Actions CI/CD
```

---

# Data Flow

### Real-Time Path (Streaming)

1. **Bid Placed** â†’ Auction API publishes event to Kafka `auction.bids` topic
2. **Spark Streaming** consumes events, validates bids, calculates fraud scores
3. **PostgreSQL** receives validated bid records (Silver layer)
4. **Application** queries PostgreSQL for current auction state

### Batch Path (Analytics)

1. **Airflow** triggers daily at 2 AM UTC
2. **Spark Batch** reads from Silver layer, applies aggregations
3. **Gold Layer** metrics written to PostgreSQL
4. **Metabase** dashboards refresh with new data

### Fraud Detection Pipeline

```
Bid Event â†’ Validate Amount â†’ Check Increment â†’ Calculate Fraud Score â†’ Flag if > 0.5
                  â”‚                  â”‚                    â”‚
                  â–¼                  â–¼                    â–¼
            Reject if          Flag unusual         Score factors:
            bid < current      increment (>50%      - Increment size
                              of previous)          - Bid timing (snipe)
                                                    - User history
```

---

# Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `KAFKA_BOOTSTRAP_SERVERS` | Kafka broker addresses | `localhost:9092` |
| `POSTGRES_HOST` | PostgreSQL host | `localhost` |
| `POSTGRES_DB` | Database name | `auction` |
| `AWS_REGION` | AWS region for S3 | `us-east-1` |
| `S3_BUCKET` | Data lake bucket name | `auction-data-lake` |

### Spark Tuning

Edit `config/spark-defaults.conf` for production workloads:

```properties
spark.executor.memory=2g
spark.executor.cores=2
spark.sql.shuffle.partitions=200
spark.streaming.kafka.maxRatePerPartition=1000
```

---

# Deployment

### AWS Free Tier Deployment

```bash
cd terraform

# Initialize Terraform
terraform init

# Preview changes
terraform plan

# Deploy infrastructure
terraform apply

# Get outputs (RDS endpoint, S3 bucket name)
terraform output
```

### Cost Estimation (Free Tier)

| Service | Monthly Cost |
|---------|--------------|
| S3 (5GB) | $0 |
| RDS db.t3.micro | $0 |
| Lambda (if used) | $0 |
| Data Transfer (< 100GB) | $0 |
| **Total** | **$0-5** |

---

# Testing

```bash
# Run all tests
make test

# Run with coverage
make test-coverage

# Run specific test file
pytest tests/test_generator.py -v

# Run integration tests (requires Docker)
make test-integration
```

---

# Monitoring

### Kafka Metrics (via Kafka UI)

- Consumer lag per partition
- Message throughput
- Topic sizes

### Spark Metrics

Access Spark UI at `http://localhost:4040` during job execution:
- Stage progress
- Memory usage
- Shuffle statistics

### Custom Dashboards (Metabase)

Pre-configured dashboards include:
- **Executive Summary**: Daily revenue, transaction counts, KPIs
- **Auction Performance**: Success rates, bid patterns, price trends
- **User Analytics**: Bidder segmentation, win rates, engagement
- **Real-Time Monitor**: Active auctions, live bids, fraud alerts

---

# Key Capabilities

| Capability | Metric | Status |
|------------|--------|--------|
| Bid Processing Throughput | 10,000+ events/second | âœ… |
| End-to-End Latency | < 500ms (streaming path) | âœ… |
| Data Freshness | Real-time for active auctions | âœ… |
| Historical Analytics | Daily batch aggregations | âœ… |
| Fraud Detection | Real-time scoring | âœ… |
| Availability Target | 99.9% | âœ… |

---

# Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

# Conclusion

Through the solution created, the main business needs for the client have been met. The pipeline is able to:

âœ… **Ingest real-time bids** from the auction web application via Kafka, handling 10,000+ events per second

âœ… **Process batch data** from physical auctions stored in S3 on a daily schedule

âœ… **Validate and transform** auction data through the medallion architecture (Bronze â†’ Silver â†’ Gold)

âœ… **Detect fraudulent patterns** in real-time using configurable scoring algorithms

âœ… **Visualize key metrics** through Metabase dashboards for business intelligence

âœ… **Deploy cost-effectively** within AWS Free Tier constraints using Terraform

The functional and non-functional requirements for this solution have been carefully considered in the design and implementation, and this pipeline can be scaled up or improved when there is a technical and practical reason to do so. The architecture decisions are documented in ADRs for future reference and onboarding.

---

**Built with â¤ï¸ for learning modern data engineering**
